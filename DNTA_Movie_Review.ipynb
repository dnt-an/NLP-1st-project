{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "DNTA-Movie Review.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MDEjcL7Lgs2",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHQ1QCGHLgs4",
        "colab_type": "text"
      },
      "source": [
        "## Basic NLP Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaTMGLDPLgs4",
        "colab_type": "text"
      },
      "source": [
        "- **Step 1**: Clean data\n",
        "    - Remove all irrelevant characters such as any non alphanumeric characters\n",
        "    - Tokenize your text by separating it into individual words \n",
        "    - Remove words that are not relevant, such as “@” twitter mentions or urls \n",
        "    - Convert all characters to **lowercase**, in order to treat words such as “hello”, “Hello”, and “HELLO” the same \n",
        "    - Consider **lemmatization** (reduce words such as “am”, “are”, and “is” to a common form such as “be”)\n",
        "- **Step 2**: Representation\n",
        "    - Bag of Words or TFIDF\n",
        "- **Step 3**: Classification\n",
        "    - Naive Bayes\n",
        "    - Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQuoh8HSOs9F",
        "colab_type": "code",
        "outputId": "b0c7a476-52c7-47a4-bed5-b4f56d489ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPtgQIuhLgtd",
        "colab_type": "code",
        "outputId": "033fda94-0d2d-4142-de31-2453533edb59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# Import pandas, numpy and the dataset, save it in a object called 'sentiment'\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Read the data\n",
        "data = pd.read_csv('/content/drive/My Drive/FTMLE - Tonga/Data/movie_review.csv', encoding='utf-8', sep='\\t')\n",
        "\n",
        "# Let's check some samples\n",
        "data.sample(10)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10402</th>\n",
              "      <td>7517_1</td>\n",
              "      <td>***One Out of Ten Stars*** &lt;br /&gt;&lt;br /&gt;Because...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750</th>\n",
              "      <td>1099_4</td>\n",
              "      <td>I was shocked by the ridiculously unbelievable...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16439</th>\n",
              "      <td>10636_8</td>\n",
              "      <td>This story is a complex and wonderful tale of ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4147</th>\n",
              "      <td>6095_1</td>\n",
              "      <td>i've seen a movie thats sort of like this, wer...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3957</th>\n",
              "      <td>3808_10</td>\n",
              "      <td>I voted this a 10 out of 10 simply because it ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16932</th>\n",
              "      <td>4780_9</td>\n",
              "      <td>Always fancied this film from the video cover....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4720</th>\n",
              "      <td>1623_10</td>\n",
              "      <td>This is te cartoon that should have won instea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7784</th>\n",
              "      <td>6752_7</td>\n",
              "      <td>A detective (Dana Andrews) with a reputation f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15418</th>\n",
              "      <td>10494_1</td>\n",
              "      <td>On Steve Irwin's show, he's hillarious. He doe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7299</th>\n",
              "      <td>8837_1</td>\n",
              "      <td>This film is terrible. I was really looking fo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id                                             review  sentiment\n",
              "10402   7517_1  ***One Out of Ten Stars*** <br /><br />Because...          0\n",
              "750     1099_4  I was shocked by the ridiculously unbelievable...          0\n",
              "16439  10636_8  This story is a complex and wonderful tale of ...          1\n",
              "4147    6095_1  i've seen a movie thats sort of like this, wer...          0\n",
              "3957   3808_10  I voted this a 10 out of 10 simply because it ...          1\n",
              "16932   4780_9  Always fancied this film from the video cover....          1\n",
              "4720   1623_10  This is te cartoon that should have won instea...          1\n",
              "7784    6752_7  A detective (Dana Andrews) with a reputation f...          1\n",
              "15418  10494_1  On Steve Irwin's show, he's hillarious. He doe...          0\n",
              "7299    8837_1  This film is terrible. I was really looking fo...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw-RQlFgLgtc",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment analysis\n",
        "The task is to build a model that will determine the tone (positive, negative) of the text. To do this, you will need to train the model on the existing data (movie_review.csv). The resulting model will have to determine the class (neutral, positive, negative) of new texts. The dataset contains the following fields:\n",
        "\n",
        "| Field name | Meaning |\n",
        "|------------|-----------|\n",
        "| ID  | id of comment|\n",
        "| review | text of reviews|\n",
        "| sentiment | sentiment (1-positive, 0-negative)|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt_3DKQexRTp",
        "colab_type": "text"
      },
      "source": [
        "* Check balanced of the sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu1sxNqww470",
        "colab_type": "code",
        "outputId": "a78a7988-1ed1-489a-de97-765c3440cc6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data['sentiment'].value_counts(normalize = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.501244\n",
              "0    0.498756\n",
              "Name: sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_dqddptLgtu",
        "colab_type": "text"
      },
      "source": [
        "### Term frequency-inverse document frequency (tf-idf)\n",
        "\n",
        "We could use raw term frequencies to score the words in our algorithm. But there is a problem though: If a word is very frequent in _all_ documents, then it probably doesn't carry a lot of information. In order to tackle this problem we can use **term frequency-inverse document frequency**, which will reduce the score the more frequent the word is accross all tweets. It is calculated like this:\n",
        "\n",
        "\\begin{equation*}\n",
        "tf-idf(t,d) = tf(t,d) ~ idf(t,d)\n",
        "\\end{equation*}\n",
        "\n",
        "_tf(t,d)_ is the raw term frequency descrived above. _idf(t,d)_ is the inverse document frequency, than can be calculated as follows:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\log \\frac{n_d}{1+df\\left(d,t\\right)}\n",
        "\\end{equation*}\n",
        "\n",
        "where `n` is the total number of documents and _df(t,d)_ is the number of documents where the term `t` appears. \n",
        "\n",
        "The `1` addition in the denominator is just to avoid zero term for terms that appear in all documents, will not be entirely ignored. Ans the `log` ensures that low frequency term don't get too much weight.\n",
        "\n",
        "Fortunately for us `scikit-learn` does all those calculations for us:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_ErR308Lgtz",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Data clean up\n",
        "\n",
        "### Removing stop words\n",
        "\n",
        "Now that we know how to format and score our input. Let's look at our **real** vocabulary. Specifically, the most common words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kAfdY7CLgt1",
        "colab_type": "code",
        "outputId": "1ff959c6-c7aa-47bc-9f34-2e8dc6271638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "from collections import Counter\n",
        "vocab = Counter()\n",
        "\n",
        "# Apply Counter to count words in our reviews\n",
        "for document in data['review']:\n",
        "  for word in document.split(' '):\n",
        "    vocab[word] += 1\n",
        "    \n",
        "# Show 20 most common words\n",
        "vocab.most_common(20)   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 258519),\n",
              " ('a', 139707),\n",
              " ('and', 137397),\n",
              " ('of', 128750),\n",
              " ('to', 119278),\n",
              " ('is', 92935),\n",
              " ('in', 77245),\n",
              " ('I', 59255),\n",
              " ('that', 57991),\n",
              " ('this', 51379),\n",
              " ('it', 48865),\n",
              " ('/><br', 45851),\n",
              " ('was', 42004),\n",
              " ('as', 38288),\n",
              " ('with', 37496),\n",
              " ('for', 36919),\n",
              " ('The', 30399),\n",
              " ('but', 30350),\n",
              " ('on', 27738),\n",
              " ('movie', 27342)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A94oAnZLgt3",
        "colab_type": "text"
      },
      "source": [
        "### Stop words\n",
        "As we can see, the most common words are meaningless in terms of sentiment: _I, to, the, and_... they don't give any information on positiveness or negativeness. They're basically **noise** that can most probably be eliminated. These kind of words are called _stop words_, and it is a common practice to remove them when doing text analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeWTMVcyLgt4",
        "colab_type": "code",
        "outputId": "cbee1925-e127-466b-b4f4-d803198c3d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSjQwJ5iLgt8",
        "colab_type": "code",
        "outputId": "c016e90b-88cc-46e7-a1bb-c6a4eb5ae0a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "vocab_reduced = Counter()\n",
        "# Go through all of the items of vocab using vocab.items() and pick only words that are not in 'stop' \n",
        "# and save them in vocab_reduced\n",
        "for word, count in vocab.items():\n",
        "  if word.lower() not in stop:\n",
        "    vocab_reduced[word] = count\n",
        "\n",
        "vocab_reduced.most_common(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('/><br', 45851),\n",
              " ('movie', 27342),\n",
              " ('film', 24768),\n",
              " ('one', 18704),\n",
              " ('like', 16278),\n",
              " ('would', 10720),\n",
              " ('good', 10243),\n",
              " ('really', 9773),\n",
              " ('even', 9530),\n",
              " ('see', 9077),\n",
              " ('-', 8181),\n",
              " ('get', 7857),\n",
              " ('story', 7652),\n",
              " ('much', 7634),\n",
              " ('time', 7028),\n",
              " ('make', 6719),\n",
              " ('could', 6700),\n",
              " ('also', 6672),\n",
              " ('people', 6604),\n",
              " ('first', 6570)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xji-S78TLgt-",
        "colab_type": "text"
      },
      "source": [
        "This looks better, only in the 20 most common words we already see words that make sense: good, love, really... "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z01BpBz7Lgt-",
        "colab_type": "text"
      },
      "source": [
        "### Removing special characters and \"trash\"\n",
        "\n",
        "If you look closer, you'll see that we're also taking into consideration punctuation signs ('-', ',', etc) and other html tags like `&amp`. We can definitely remove them for the sentiment analysis, but we will try to keep the emoticons, since those _do_ have a sentiment load:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9pqiFUXLgt_",
        "colab_type": "code",
        "outputId": "b740d9c9-9059-400b-c2d7-c19c89080bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocessor(text):\n",
        "    \"\"\" Return a cleaned version of text\n",
        "    \"\"\"\n",
        "    # Remove HTML markup\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    # Save emoticons for later appending\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    # Remove any non-word character and append the emoticons,\n",
        "    # removing the noise characters for standarization. Convert to lower case\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Create some random texts for testing the function preprocessor()\n",
        "print(preprocessor('/> This is, our result. :)) <br>'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " this is our result  :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6MBLpHrd-Z-",
        "colab_type": "text"
      },
      "source": [
        "We can see that many words such as “ask” and “asked” are just different tenses of the same verb. These words will make parse vectors require more memory and computational resources when modeling and the vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms.\n",
        "\n",
        "Reducing different forms of a word to a core root is essential. It help to shorter the word parse vectors. Words that are derived from one another can be mapped to a central word or symbol, especially if they have the same core meaning.\n",
        "\n",
        "This is where something like stemming or lemmatization comes in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6mimtiSLguB",
        "colab_type": "text"
      },
      "source": [
        "We are almost ready! There is another trick we can use to reduce our vocabulary and consolidate words. If you think about it, words like: love, loving, etc. _Could_ express the same positivity. If that was the case, we would be  having two words in our vocabulary when we could have only one: lov. This process of reducing a word to its root is called **stemming**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbOCdDgiaA6q",
        "colab_type": "text"
      },
      "source": [
        "### Stemming\n",
        "With stemming, words are reduced to their word stems. A word stem need not be the same root as a dictionary-based morphological root, it just is an equal to or smaller form of the word.\n",
        "\n",
        "Ex: love, loved, loving -> love"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fum1Igsm7Pf",
        "colab_type": "code",
        "outputId": "f82f88e7-21a0-4a7a-c3ed-cd53fe57c41f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zrr34DRLguB",
        "colab_type": "code",
        "outputId": "17a4c397-ffde-483a-f7ca-2949506b7113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We are going to use Snowball stemmer this task\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "snowball = SnowballStemmer(language='english')\n",
        "\n",
        "def tokenizer_snowball(text):\n",
        "    \"\"\" Split a text into list of words and apply stemming technic \"\"\"\n",
        "\n",
        "    return [snowball.stem(word) for word in nltk.word_tokenize(text)]\n",
        "\n",
        "# Testing\n",
        "print(tokenizer_snowball('The striped bats are hanging on their feet for best'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mct7SX6a5_hC",
        "colab_type": "text"
      },
      "source": [
        "### Lemmatization\n",
        "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.<br>\n",
        "Example:<br>\n",
        "- rocks -> rock<br>\n",
        "- corpora -> corpus<br>\n",
        "- better -> good<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgq9WShIOTNm",
        "colab_type": "code",
        "outputId": "045d953a-2fe3-4b47-b132-775e7490a9e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0]\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def tokenizer_lemma(text):\n",
        "    return [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in nltk.word_tokenize(text)]\n",
        "\n",
        "# Testing\n",
        "print(tokenizer_lemma('The striped bats are hanging on their feet for best'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpOHUj0QLguG",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsZUZT0ILguH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def tokenizer_snowball(text):\n",
        "    return [snowball.stem(word) for word in text.split()]\n",
        "\n",
        "def tokenizer_lemma(text):\n",
        "    return [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in text.split()]\n",
        "\n",
        "def preprocessor(text):\n",
        "    \"\"\" Return a cleaned version of text \"\"\"\n",
        "\n",
        "    # Remove HTML markup\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    # Save emoticons for later appending\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    # Remove any non-word character and append the emoticons,\n",
        "    # removing the noise character for standarization. Convert to lower case\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0dMNzB0wpbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = TfidfVectorizer(stop_words=stop,\n",
        "                        tokenizer=tokenizer_lemma,\n",
        "                        preprocessor=preprocessor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96CGvWnKLguJ",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Classification\n",
        "\n",
        "We are finally ready to train our algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quMv4bXNLguD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the dataset in train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data['review']\n",
        "y = data['sentiment']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMA_qTW5LguJ",
        "colab_type": "code",
        "outputId": "24c6e020-c6ee-410d-d248-8face7497bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# A pipeline is what chains several steps together, once the initial exploration is done. \n",
        "# For example, some codes are meant to transform features — normalise numericals, or turn text into vectors, \n",
        "# or fill up missing data, they are transformers; other codes are meant to predict variables by fitting an algorithm,\n",
        "# they are estimators. Pipeline chains all these together which can then be applied to training data\n",
        "clf = Pipeline([('vect', tfidf),\n",
        "                ('clf', LogisticRegression(random_state=0))])\n",
        "\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=<function preprocessor at 0x7f40c98422f0>,\n",
              "                                 smooth_idf=True,\n",
              "                                 stop_words=['i', 'me', 'my', 'myself', '...\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=<function tokenizer_lemma at 0x7f40c98427b8>,\n",
              "                                 use_idf=True, vocabulary=None)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=0,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEO1d1f62HhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "pickle.dump(clf, open(os.path.join('/content/drive/My Drive/NLP_Model/lemma.pkl'), 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AONa9gWQ2NeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#snowball_model = pickle.load(open('/content/drive/My Drive/NLP_Model/snowball.pkl', 'rb'))\n",
        "lemma_model = pickle.load(open('/content/drive/My Drive/NLP_Model/lemma.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwEPS1dT7Lmp",
        "colab_type": "code",
        "outputId": "7a8e322d-a684-46f8-f898-3adce5dddab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "#snowball_predictions = snowball_model.predict(X_test)\n",
        "lemma_predictions = lemma_model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yonP8E3vLguL",
        "colab_type": "code",
        "outputId": "09b6cbc5-2ffe-4324-a531-eb7f3ea5fb7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Let's compare results between the models\n",
        "print('SNOWBALL MODEL')\n",
        "print(f'Accuracy: %s' % accuracy_score(y_test, snowball_predictions))\n",
        "print(f'Confusion matrix:' '\\n', classification_report(y_test, snowball_predictions))\n",
        "print('---------------------------------')\n",
        "print('LEMMA MODEL')\n",
        "print(f'Accuracy: %s' % accuracy_score(y_test, lemma_predictions))\n",
        "print('Confusion matrix:' '\\n', classification_report(y_test, lemma_predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SNOWBALL MODEL\n",
            "Accuracy: 0.8911111111111111\n",
            "Confusion matrix:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89      2218\n",
            "           1       0.89      0.90      0.89      2282\n",
            "\n",
            "    accuracy                           0.89      4500\n",
            "   macro avg       0.89      0.89      0.89      4500\n",
            "weighted avg       0.89      0.89      0.89      4500\n",
            "\n",
            "---------------------------------\n",
            "LEMMA MODEL\n",
            "Accuracy: 0.888\n",
            "Confusion matrix:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89      2218\n",
            "           1       0.89      0.89      0.89      2282\n",
            "\n",
            "    accuracy                           0.89      4500\n",
            "   macro avg       0.89      0.89      0.89      4500\n",
            "weighted avg       0.89      0.89      0.89      4500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw4CNXsETq0h",
        "colab_type": "text"
      },
      "source": [
        "### Import test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txbL40AOTfu2",
        "colab_type": "code",
        "outputId": "f5b5939f-41de-4efb-85b0-df43c3a6da86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "test_data = pd.read_csv('/content/drive/My Drive/FTMLE - Tonga/Data/movie_review_evaluation.csv', encoding='utf-8', sep='\\t')\n",
        "test_data.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10633_1</td>\n",
              "      <td>I watched this video at a friend's house. I'm ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4489_1</td>\n",
              "      <td>`The Matrix' was an exciting summer blockbuste...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3304_10</td>\n",
              "      <td>This movie is one among the very few Indian mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3350_3</td>\n",
              "      <td>The script for this movie was probably found i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1119_1</td>\n",
              "      <td>Even if this film was allegedly a joke in resp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                             review\n",
              "0  10633_1  I watched this video at a friend's house. I'm ...\n",
              "1   4489_1  `The Matrix' was an exciting summer blockbuste...\n",
              "2  3304_10  This movie is one among the very few Indian mo...\n",
              "3   3350_3  The script for this movie was probably found i...\n",
              "4   1119_1  Even if this film was allegedly a joke in resp..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRMIty1hOEIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using lemmanization model to predict test data\n",
        "predictions = lemma_model.predict(test_data['review'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtfnoUPOOWdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = {'review':test_data['review'], 'sentiment':predictions}\n",
        "result = pd.DataFrame(d)\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28wFPaU25JIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result.to_csv('andang.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}